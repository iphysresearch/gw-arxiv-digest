#!/usr/bin/env python3
"""
arXiv ç½‘é¡µçˆ¬è™«æ¨¡å— - ç›´æ¥ä» arXiv ç½‘é¡µçˆ¬å–è®ºæ–‡ä¿¡æ¯
æ›¿ä»£ arxiv åº“çš„åŠŸèƒ½
"""

import requests
import re
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional    
from urllib.parse import urljoin
import datetime
from dataclasses import dataclass

@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç±»ï¼Œæ¨¡æ‹Ÿ arxiv åº“çš„ Result å¯¹è±¡ç»“æ„"""
    entry_id: str
    title: str
    authors: List[Dict[str, str]]  # [{"name": "Author Name"}, ...]
    summary: str
    pdf_url: str
    published: Optional[datetime.datetime] = None
    updated: Optional[datetime.datetime] = None
    categories: List[str] = None
    primary_category: str = ""

    def __post_init__(self):
        if self.categories is None:
            self.categories = []

class ArxivWebScraper:
    """arXiv ç½‘é¡µçˆ¬è™«ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.base_url = "https://arxiv.org"
    
    def fetch_category_new(self, category: str) -> tuple[List[Paper], dict]:
        """
        è·å–ç‰¹å®šç±»åˆ«çš„æ–°è®ºæ–‡ (New submissions)
        Args:
            category: ç±»åˆ«åç§°ï¼Œå¦‚ 'gr-qc', 'astro-ph.CO' ç­‰
        Returns:
            tuple[List[Paper], dict]: (è®ºæ–‡åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯)
        """
        url = f"{self.base_url}/list/{category}/new"
        print(f"ğŸ” æ­£åœ¨çˆ¬å– {url}")
        
        stats = {
            "category": category,
            "url": url,
            "expected_total": 0,
            "actual_crawled": 0,
            "page_source_info": "",
            "success": False,
            "verification_passed": False
        }
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–é¡µé¢æ€»æ•°ä¿¡æ¯å¹¶è¿›è¡Œè‡ªæ£€
            expected_total, page_info = self._extract_total_count(soup)
            stats["expected_total"] = expected_total
            stats["page_source_info"] = page_info
            
            # è§£æè®ºæ–‡
            papers = self._parse_arxiv_list_page(soup)
            stats["actual_crawled"] = len(papers)
            stats["success"] = True
            
            # éªŒè¯çˆ¬å–æ•°é‡æ˜¯å¦åŒ¹é…
            if expected_total > 0:
                # å…è®¸ä¸€å®šçš„å®¹é”™èŒƒå›´ï¼ˆÂ±5ç¯‡ï¼‰
                tolerance = 5
                if abs(expected_total - len(papers)) <= tolerance:
                    stats["verification_passed"] = True
                    print(f"âœ… éªŒè¯é€šè¿‡: é¡µé¢æ˜¾ç¤º {expected_total} ç¯‡ï¼Œå®é™…çˆ¬å– {len(papers)} ç¯‡")
                else:
                    stats["verification_passed"] = False
                    print(f"âš ï¸ æ•°é‡ä¸åŒ¹é…: é¡µé¢æ˜¾ç¤º {expected_total} ç¯‡ï¼Œå®é™…çˆ¬å– {len(papers)} ç¯‡")
            else:
                print(f"âš ï¸ æ— æ³•è·å–é¡µé¢æ€»æ•°ï¼Œå®é™…çˆ¬å– {len(papers)} ç¯‡")
            
            return papers, stats
            
        except requests.RequestException as e:
            print(f"âŒ çˆ¬å– {category} å¤±è´¥: {e}")
            stats["error"] = str(e)
            return [], stats
    
    def fetch_category_recent(self, category: str, days: int = 5) -> tuple[List[Paper], dict]:
        """
        è·å–ç‰¹å®šç±»åˆ«çš„æœ€è¿‘è®ºæ–‡ (Recent submissions, Cross-lists, Replacements)
        """
        url = f"{self.base_url}/list/{category}/recent"
        print(f"ğŸ” æ­£åœ¨çˆ¬å–æœ€è¿‘è®ºæ–‡ {url}")
        
        stats = {
            "category": category,
            "url": url,
            "expected_total": 0,
            "actual_crawled": 0,
            "page_source_info": "Recent submissions page",
            "success": False,
            "verification_passed": True  # Recent é¡µé¢é€šå¸¸ä¸æ˜¾ç¤ºæ€»æ•°
        }
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            papers = self._parse_arxiv_list_page(soup)
            
            stats["actual_crawled"] = len(papers)
            stats["success"] = True
            
            return papers, stats
            
        except requests.RequestException as e:
            print(f"âŒ çˆ¬å– {category} æœ€è¿‘è®ºæ–‡å¤±è´¥: {e}")
            stats["error"] = str(e)
            return [], stats
    
    def _extract_total_count(self, soup: BeautifulSoup) -> tuple[int, str]:
        """
        æå–é¡µé¢æ€»æ•°ä¿¡æ¯å¹¶è¿›è¡Œè‡ªæ£€
        Returns:
            tuple[int, str]: (æ€»æ•°, åŸå§‹æ–‡æœ¬)
        """
        total_count = 0
        original_text = ""
        
        # æ–¹æ³•1: æŸ¥æ‰¾ paging div ä¸­çš„ä¿¡æ¯
        paging_div = soup.find('div', class_='paging')
        if paging_div:
            text = paging_div.get_text()
            match = re.search(r'Total of (\d+) entries', text)
            if match:
                total_count = int(match.group(1))
                original_text = text.strip()
                print(f"ğŸ“Š åœ¨ paging div ä¸­æ‰¾åˆ°æ€»æ•°: {total_count}")
                return total_count, original_text
        
        # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å« "Total of X entries" çš„ä»»ä½•æ–‡æœ¬
        total_pattern = re.compile(r'Total of (\d+) entries', re.IGNORECASE)
        for element in soup.find_all(string=total_pattern):
            match = total_pattern.search(element)
            if match:
                total_count = int(match.group(1))
                original_text = element.strip()
                print(f"ğŸ“Š åœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°æ€»æ•°: {total_count}")
                return total_count, original_text
        
        # æ–¹æ³•3: æŸ¥æ‰¾ä»»ä½•åŒ…å«æ•°å­—å’Œ entries çš„æ–‡æœ¬
        for text in soup.find_all(string=True):
            if 'entries' in text.lower() and re.search(r'\d+', text):
                numbers = re.findall(r'\d+', text)
                if numbers:
                    total_count = int(numbers[0])  # å–ç¬¬ä¸€ä¸ªæ•°å­—
                    original_text = text.strip()
                    print(f"ğŸ“Š é€šè¿‡æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°æ€»æ•°: {total_count}")
                    return total_count, original_text
        
        print("âš ï¸ æœªæ‰¾åˆ°é¡µé¢æ€»æ•°ä¿¡æ¯")
        return 0, "æœªæ‰¾åˆ°æ€»æ•°ä¿¡æ¯"
    
    def _parse_arxiv_list_page(self, soup: BeautifulSoup) -> List[Paper]:
        """è§£æ arXiv åˆ—è¡¨é¡µé¢"""
        papers = []
        
        # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ¡ç›® - arXiv ä½¿ç”¨ <dt> å’Œ <dd> æ ‡ç­¾å¯¹
        dt_elements = soup.find_all('dt')
        dd_elements = soup.find_all('dd')
        
        print(f"ğŸ” æ‰¾åˆ° {len(dt_elements)} ä¸ª <dt> å…ƒç´ ï¼Œ{len(dd_elements)} ä¸ª <dd> å…ƒç´ ")
        
        # ç¡®ä¿ dt å’Œ dd æ•°é‡åŒ¹é…
        min_count = min(len(dt_elements), len(dd_elements))
        
        for i in range(min_count):
            try:
                dt = dt_elements[i]
                dd = dd_elements[i]
                
                paper = self._parse_paper_entry(dt, dd)
                if paper:
                    papers.append(paper)
                    
            except Exception as e:
                print(f"âš ï¸ è§£æè®ºæ–‡æ¡ç›® {i+1} å¤±è´¥: {e}")
                continue
        
        print(f"âœ… æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    def _parse_paper_entry(self, dt_element, dd_element) -> Optional[Paper]:
        """è§£æå•ä¸ªè®ºæ–‡æ¡ç›®"""
        try:
            # ä» dt å…ƒç´ æå– arXiv ID å’Œæäº¤ç±»å‹ä¿¡æ¯
            arxiv_id, submission_info = self._extract_arxiv_id(dt_element)
            if not arxiv_id:
                return None
            
            # ä» dd å…ƒç´ æå–è¯¦ç»†ä¿¡æ¯
            title = self._extract_title(dd_element)
            authors = self._extract_authors(dd_element)
            summary = self._extract_summary(dd_element)
            categories, primary_category = self._extract_categories(dd_element)
            dates = self._extract_dates(dd_element)
            
            # æ„å»º PDF URL
            pdf_url = f"{self.base_url}/pdf/{arxiv_id}.pdf"
            entry_id = f"{self.base_url}/abs/{arxiv_id}"
            
            # åˆ›å»ºè®ºæ–‡å¯¹è±¡
            paper = Paper(
                entry_id=entry_id,
                title=title,
                authors=authors,
                summary=summary,
                pdf_url=pdf_url,
                published=dates.get('published'),
                updated=dates.get('updated'),
                categories=categories,
                primary_category=primary_category
            )
            
            # æ·»åŠ æäº¤ç±»å‹ä¿¡æ¯ä½œä¸ºå±æ€§
            paper.submission_info = submission_info
            
            return paper
            
        except Exception as e:
            print(f"âš ï¸ è§£æè®ºæ–‡è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _extract_arxiv_id(self, dt_element) -> tuple[Optional[str], dict]:
        """
        æå– arXiv ID å’Œæäº¤ç±»å‹ä¿¡æ¯
        Returns:
            tuple[Optional[str], dict]: (arXiv_id, submission_info)
        """
        text = dt_element.get_text()
        submission_info = {"type": "new", "details": "", "cross_list_from": ""}
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… arXiv ID
        arxiv_pattern = r'arXiv:(\d{4}\.\d{4,5})'
        match = re.search(arxiv_pattern, text)
        
        arxiv_id = None
        if match:
            arxiv_id = match.group(1)
        else:
            # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾é“¾æ¥
            links = dt_element.find_all('a', href=True)
            for link in links:
                href = link['href']
                if '/abs/' in href:
                    arxiv_id = href.split('/abs/')[-1]
                    break
        
        # è§£ææäº¤ç±»å‹ä¿¡æ¯
        if arxiv_id:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ replaced
            if '(replaced)' in text.lower():
                submission_info["type"] = "replaced"
                submission_info["details"] = "replaced"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ cross-list
            cross_list_match = re.search(r'\(cross-list from ([^)]+)\)', text, re.IGNORECASE)
            if cross_list_match:
                submission_info["type"] = "cross-list"
                submission_info["cross_list_from"] = cross_list_match.group(1)
                submission_info["details"] = f"cross-list from {cross_list_match.group(1)}"
            
            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹æ®Šæ ‡è®°ï¼Œé»˜è®¤ä¸ºæ–°æäº¤
            if submission_info["type"] == "new" and submission_info["details"] == "":
                submission_info["details"] = "new submission"
        
        return arxiv_id, submission_info
    
    def _extract_title(self, dd_element) -> str:
        """æå–æ ‡é¢˜"""
        # æŸ¥æ‰¾ Title: åé¢çš„å†…å®¹
        title_div = dd_element.find('div', class_='list-title')
        if title_div:
            title_text = title_div.get_text()
            # ç§»é™¤ "Title: " å‰ç¼€
            title = re.sub(r'^Title:\s*', '', title_text, flags=re.IGNORECASE).strip()
            return title
        
        # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥ä»æ–‡æœ¬ä¸­æå–
        text = dd_element.get_text()
        lines = text.split('\n')
        
        # æ ‡é¢˜é€šå¸¸åœ¨ç¬¬ä¸€è¡Œæˆ–ç¬¬äºŒè¡Œ
        for line in lines[:3]:
            line = line.strip()
            if line and not line.startswith(('Authors:', 'Comments:', 'Subjects:', 'Journal-ref:')):
                # ç§»é™¤å¯èƒ½çš„ "Title:" å‰ç¼€
                title = re.sub(r'^Title:\s*', '', line, flags=re.IGNORECASE).strip()
                if title and len(title) > 10:  # ç¡®ä¿æ˜¯æœ‰æ„ä¹‰çš„æ ‡é¢˜
                    return title
        
        return "æœªæ‰¾åˆ°æ ‡é¢˜"
    
    def _extract_authors(self, dd_element) -> List[Dict[str, str]]:
        """æå–ä½œè€…åˆ—è¡¨"""
        # æŸ¥æ‰¾ä½œè€…ä¿¡æ¯
        text = dd_element.get_text()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ä½œè€…éƒ¨åˆ†
        # arXiv æ ¼å¼é€šå¸¸æ˜¯ï¼šTitle\nä½œè€…åå•\nComments: ... æˆ– Subjects: ...
        lines = text.split('\n')
        
        authors_line = ""
        for i, line in enumerate(lines):
            line = line.strip()
            if line and not line.startswith(('Title:', 'Comments:', 'Subjects:', 'Journal-ref:')):
                # å¦‚æœè¿™ä¸æ˜¯ç¬¬ä¸€è¡Œï¼ˆæ ‡é¢˜ï¼‰ï¼Œå¯èƒ½æ˜¯ä½œè€…è¡Œ
                if i > 0:
                    # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯ Comments æˆ– Subjectsï¼Œå¦‚æœæ˜¯ï¼Œå½“å‰è¡Œå¯èƒ½æ˜¯ä½œè€…
                    next_line = lines[i+1].strip() if i+1 < len(lines) else ""
                    if next_line.startswith(('Comments:', 'Subjects:', 'Journal-ref:')) or i == len(lines) - 1:
                        authors_line = line
                        break
        
        # è§£æä½œè€…åå•
        authors = []
        if authors_line:
            # æ¸…ç†ä½œè€…è¡Œï¼Œç§»é™¤æœºæ„ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æ‹¬å·å†…ï¼‰
            # ä½†ä¿ç•™åŸºæœ¬çš„å§“åä¿¡æ¯
            authors_clean = re.sub(r'\([^)]*\)', '', authors_line)  # ç§»é™¤æ‹¬å·å†…å®¹
            
            # åˆ†å‰²ä½œè€…åå­— - é€šå¸¸ä»¥é€—å·åˆ†éš”
            author_names = re.split(r',\s*(?![^(]*\))', authors_clean)  # ä¸åœ¨æ‹¬å·å†…çš„é€—å·
            
            for name in author_names:
                name = name.strip()
                if name and len(name) > 1:  # è¿‡æ»¤æ‰å•å­—ç¬¦
                    # è¿›ä¸€æ­¥æ¸…ç†åå­—
                    name = re.sub(r'^\d+\.\s*', '', name)  # ç§»é™¤ç¼–å·
                    name = re.sub(r'\s+', ' ', name)  # è§„èŒƒåŒ–ç©ºæ ¼
                    if name and len(name) > 2:
                        authors.append({"name": name})
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä½œè€…ï¼Œå°è¯•å¦ä¸€ç§æ–¹æ³•
        if not authors:
            # æŸ¥æ‰¾åŒ…å«äººåæ¨¡å¼çš„è¡Œ
            for line in lines:
                line = line.strip()
                # ç®€å•çš„äººåæ¨¡å¼åŒ¹é…ï¼ˆåŒ…å«å¤§å†™å­—æ¯å¼€å¤´çš„å•è¯ï¼‰
                if re.search(r'^[A-Z][a-z]+\s+[A-Z]', line) and not line.startswith(('Title:', 'Comments:', 'Subjects:')):
                    authors = [{"name": line}]
                    break
        
        return authors if authors else [{"name": "Unknown Author"}]
    
    def _extract_summary(self, dd_element) -> str:
        """æå–æ‘˜è¦"""
        text = dd_element.get_text()
        lines = text.split('\n')
        
        # æ‘˜è¦é€šå¸¸æ˜¯æœ€åçš„ä¸€å¤§æ®µæ–‡æœ¬ï¼Œä¸åŒ…å«ç‰¹å®šçš„å‰ç¼€
        summary_lines = []
        in_summary = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # è·³è¿‡æ ‡é¢˜ã€ä½œè€…ã€è¯„è®ºã€å­¦ç§‘ç­‰å…ƒæ•°æ®
            if line.startswith(('Title:', 'Authors:', 'Comments:', 'Subjects:', 'Journal-ref:', 'DOI:', 'Report-no:')):
                in_summary = False
                continue
            
            # å¦‚æœæ˜¯å…ƒæ•°æ®è¡Œä¹‹åçš„å†…å®¹ï¼Œå¯èƒ½æ˜¯æ‘˜è¦
            if not in_summary and line and len(line) > 30:  # å‡è®¾æ‘˜è¦è¡Œæ¯”è¾ƒé•¿
                in_summary = True
            
            if in_summary:
                summary_lines.append(line)
        
        summary = ' '.join(summary_lines).strip()
        
        # å¦‚æœæ²¡æ‰¾åˆ°æ˜æ˜¾çš„æ‘˜è¦ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        if not summary or len(summary) < 50:
            # æŸ¥æ‰¾æœ€é•¿çš„æ–‡æœ¬æ®µè½
            max_line = ""
            for line in lines:
                line = line.strip()
                if len(line) > len(max_line) and not line.startswith(('Title:', 'Authors:', 'Comments:', 'Subjects:', 'Journal-ref:')):
                    max_line = line
            summary = max_line
        
        return summary if summary else "æœªæ‰¾åˆ°æ‘˜è¦"
    
    def _extract_categories(self, dd_element) -> tuple[List[str], str]:
        """æå–ç±»åˆ«ä¿¡æ¯"""
        text = dd_element.get_text()
        
        # æŸ¥æ‰¾ Subjects: è¡Œ
        subjects_match = re.search(r'Subjects:\s*(.+?)(?:\n|$)', text, re.IGNORECASE)
        if not subjects_match:
            return [], ""
        
        subjects_text = subjects_match.group(1).strip()
        
        # è§£æç±»åˆ«
        categories = []
        primary_category = ""
        
        # æ¸…ç† HTML æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
        subjects_clean = re.sub(r'<[^>]+>', '', subjects_text)
        
        # åˆ†å‰²ç±»åˆ« - é€šå¸¸ä»¥åˆ†å·æˆ–é€—å·åˆ†éš”
        parts = re.split(r'[;,]\s*', subjects_clean)
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
                
            # æå–ç±»åˆ«ä»£ç 
            # æŸ¥æ‰¾æ¨¡å¼å¦‚ "General Relativity and Quantum Cosmology (gr-qc)"
            cat_match = re.search(r'\(([a-z-]+(?:\.[A-Z]{2})?)\)', part)
            if cat_match:
                cat_code = cat_match.group(1)
                categories.append(cat_code)
                if not primary_category:
                    primary_category = cat_code
            else:
                # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œå°è¯•ç›´æ¥åŒ¹é…ç±»åˆ«ä»£ç 
                direct_match = re.search(r'^([a-z-]+(?:\.[A-Z]{2})?)(?:\s|$)', part)
                if direct_match:
                    cat_code = direct_match.group(1)
                    categories.append(cat_code)
                    if not primary_category:
                        primary_category = cat_code
        
        return categories, primary_category
    
    def _extract_dates(self, dd_element) -> Dict[str, Optional[datetime.datetime]]:
        """æå–å‘å¸ƒå’Œæ›´æ–°æ—¥æœŸ"""
        dates = {"published": None, "updated": None}
        
        text = dd_element.get_text()
        
        # æŸ¥æ‰¾æäº¤æ—¥æœŸ - å¤šç§å¯èƒ½çš„æ ¼å¼
        date_patterns = [
            r'Submitted on\s+(\d{1,2}\s+\w{3}\s+\d{4})',
            r'submitted\s+(\d{1,2}\s+\w{3}\s+\d{4})',
            r'\[(\d{1,2}\s+\w{3}\s+\d{4})\]',
            r'(\d{1,2}\s+\w{3}\s+\d{4})'  # ç›´æ¥çš„æ—¥æœŸæ ¼å¼
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    date_str = match.group(1)
                    dates["published"] = datetime.datetime.strptime(date_str, "%d %b %Y")
                    break
                except ValueError:
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®æ—¥æœŸï¼Œä½¿ç”¨ä»Šå¤©ä½œä¸ºé»˜è®¤å€¼
        if not dates["published"]:
            dates["published"] = datetime.datetime.now()
        
        return dates

# å‘åå…¼å®¹æ€§ï¼šåˆ›å»ºä¸ arxiv åº“ç±»ä¼¼çš„æ¥å£
class Client:
    """æ¨¡æ‹Ÿ arxiv.Client çš„æ¥å£"""
    
    def __init__(self):
        self.scraper = ArxivWebScraper()
    
    def results(self, search) -> List[Paper]:
        """æ¨¡æ‹Ÿ arxiv åº“çš„ results æ–¹æ³•"""
        return search.execute(self.scraper)

class Search:
    """æ¨¡æ‹Ÿ arxiv.Search çš„æ¥å£"""
    
    def __init__(self, query: str, max_results: int = 10, **kwargs):
        self.query = query
        self.max_results = max_results
        self.sort_by = kwargs.get('sort_by')
        self.sort_order = kwargs.get('sort_order')
    
    def execute(self, scraper: ArxivWebScraper) -> List[Paper]:
        """æ‰§è¡Œæœç´¢"""
        papers = []
        all_stats = []
        
        # è§£ææŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œæå–ç±»åˆ«
        categories = self._parse_query_categories()
        
        for category in categories:
            # è·å–æ–°è®ºæ–‡
            new_papers, new_stats = scraper.fetch_category_new(category)
            papers.extend(new_papers)
            all_stats.append(new_stats)
            
            # å¦‚æœéœ€è¦æ›´å¤šç»“æœï¼Œä¹Ÿè·å–æœ€è¿‘è®ºæ–‡
            if len(papers) < self.max_results:
                recent_papers, recent_stats = scraper.fetch_category_recent(category)
                papers.extend(recent_papers)
                all_stats.append(recent_stats)
        
        # å»é‡ï¼ˆåŸºäº entry_idï¼‰
        unique_papers = {}
        for paper in papers:
            paper_id = paper.entry_id.split('/')[-1]
            if paper_id not in unique_papers:
                unique_papers[paper_id] = paper
        
        papers = list(unique_papers.values())
        
        # é™åˆ¶ç»“æœæ•°é‡
        if len(papers) > self.max_results:
            papers = papers[:self.max_results]
        
        # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
        if hasattr(scraper, 'last_stats'):
            scraper.last_stats = all_stats
        
        return papers
    
    def _parse_query_categories(self) -> List[str]:
        """è§£ææŸ¥è¯¢å­—ç¬¦ä¸²ä¸­çš„ç±»åˆ«"""
        categories = []
        
        # åŒ¹é… cat:category æ ¼å¼
        cat_matches = re.findall(r'cat:([a-z-]+(?:\.[A-Z]{2})?(?:\.\*)?)', self.query)
        
        for match in cat_matches:
            if match.endswith('.*'):
                # å¤„ç†é€šé…ç¬¦ï¼Œå¦‚ astro-ph.*
                base_cat = match[:-2]  # ç§»é™¤ .*
                if base_cat == 'astro-ph':
                    # æ·»åŠ æ‰€æœ‰ astro-ph å­ç±»åˆ«
                    categories.extend([
                        'astro-ph.CO',  # å®‡å®™å­¦
                        'astro-ph.HE',  # é«˜èƒ½å¤©ä½“ç‰©ç†
                        'astro-ph.GA',  # é“¶æ²³ç³»å¤©ä½“ç‰©ç†
                        'astro-ph.SR',  # æ’æ˜Ÿç‰©ç†
                        'astro-ph.IM',  # ä»ªå™¨å’Œæ–¹æ³•
                        'astro-ph.EP'   # åœ°å¤–è¡Œæ˜Ÿ
                    ])
                else:
                    categories.append(base_cat)
            else:
                categories.append(match)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç±»åˆ«ï¼Œé»˜è®¤ä½¿ç”¨ gr-qc
        if not categories:
            categories = ['gr-qc']
        
        return categories

# å…¼å®¹æ€§æšä¸¾ç±»
class SortCriterion:
    SubmittedDate = "submittedDate"
    LastUpdatedDate = "lastUpdatedDate"
    Relevance = "relevance"

class SortOrder:
    Ascending = "ascending" 
    Descending = "descending"

# å¯¼å‡ºæ¥å£ï¼Œä¿æŒä¸ arxiv åº“çš„å…¼å®¹æ€§
__all__ = ['Client', 'Search', 'Paper', 'SortCriterion', 'SortOrder', 'ArxivWebScraper']
