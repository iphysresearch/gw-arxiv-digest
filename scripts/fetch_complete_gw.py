#!/usr/bin/env python3
"""
å®Œæ•´å¼•åŠ›æ³¢è®ºæ–‡çˆ¬å–è„šæœ¬ - åˆ†åˆ«çˆ¬å– GR-QC å’Œ Astro-Ph ç¡®ä¿è·å–å®Œæ•´æ•°æ®
ä½¿ç”¨ç½‘é¡µçˆ¬è™«ä»£æ›¿ arxiv åº“
"""

# ä½¿ç”¨æœ¬åœ°çš„ç½‘é¡µçˆ¬è™«æ¨¡å—ä»£æ›¿ arxiv åº“
from arxiv_web_scraper import Client, Search, SortCriterion, SortOrder
import datetime
import os
import json
from pathlib import Path
from typing import List, Any

# å°è¯•åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… å·²åŠ è½½ .env æ–‡ä»¶")
except ImportError:
    print("âš ï¸ python-dotenv æœªå®‰è£…ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")

# é…ç½®
ARXIV_MAX_RESULTS = int(os.getenv('ARXIV_MAX_RESULTS', '300'))
ARCHIVE_DIR = os.getenv('ARCHIVE_DIR', 'archives/complete')
ENABLE_ARCHIVE = os.getenv('ENABLE_ARCHIVE', 'true').lower() == 'true'
MATTERMOST_WEBHOOK_URL = os.getenv('MATTERMOST_WEBHOOK_URL')
MATTERMOST_MAX_PAPERS = int(os.getenv('MATTERMOST_MAX_PAPERS', '20'))

def fetch_category_papers(category: str, max_results: int = 200):
    """è·å–ç‰¹å®šç±»åˆ«çš„è®ºæ–‡ï¼ŒåŒ…å«è‡ªæ£€åŠŸèƒ½"""
    print(f"ğŸ“¡ æ­£åœ¨è·å– {category} ç±»åˆ«çš„æ–‡ç« ...")
    
    from arxiv_web_scraper import ArxivWebScraper
    scraper = ArxivWebScraper()
    
    # ç›´æ¥è·å–æ–°è®ºæ–‡å’Œç»Ÿè®¡ä¿¡æ¯
    papers, stats = scraper.fetch_category_new(category)
    
    print(f"âœ… {category}: è·å–åˆ° {len(papers)} ç¯‡æ–‡ç« ")
    
    # æ˜¾ç¤ºéªŒè¯ç»“æœ
    if stats.get("verification_passed"):
        print(f"   âœ… è‡ªæ£€é€šè¿‡: é¡µé¢æ˜¾ç¤º {stats['expected_total']} ç¯‡ï¼Œå®é™…çˆ¬å– {stats['actual_crawled']} ç¯‡")
    elif stats.get("expected_total", 0) > 0:
        print(f"   âš ï¸ è‡ªæ£€å¼‚å¸¸: é¡µé¢æ˜¾ç¤º {stats['expected_total']} ç¯‡ï¼Œå®é™…çˆ¬å– {stats['actual_crawled']} ç¯‡")
    else:
        print(f"   âš ï¸ æ— æ³•è·å–é¡µé¢æ€»æ•°è¿›è¡ŒéªŒè¯")
    
    return papers, stats

def filter_today_papers(papers):
    """ç­›é€‰ä»Šå¤©çš„æ–‡ç« ï¼ˆåŒ…æ‹¬ New submissions, Cross-lists, Replacementsï¼‰"""
    today = datetime.date.today()
    yesterday = today - datetime.timedelta(days=1)
    recent_dates = [today, yesterday]  # åŒ…æ‹¬æ˜¨å¤©ä»¥æ•è·è·¨æ—¶åŒºçš„æ–‡ç« 
    
    today_papers = []
    
    for paper in papers:
        if paper.published:
            pub_date = paper.published.date()
            upd_date = paper.updated.date() if paper.updated else pub_date
            
            # å¦‚æœå‘å¸ƒæ—¥æœŸæˆ–æ›´æ–°æ—¥æœŸåœ¨æœ€è¿‘èŒƒå›´å†…
            if pub_date in recent_dates or upd_date in recent_dates:
                today_papers.append(paper)
    
    return today_papers

def filter_gravitational_wave_papers(papers):
    """ç­›é€‰å¼•åŠ›æ³¢ç›¸å…³è®ºæ–‡"""
    filtered_papers = []
    
    for paper in papers:
        title_text = paper.title.lower()
        abstract_text = paper.summary.lower()
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å« "wave"
        if 'wave' in title_text or 'wave' in abstract_text:
            combined_text = f"{title_text} {abstract_text}"
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼•åŠ›æ³¢ç›¸å…³
            gw_patterns = [
                'gravitational wave', 'gravitational waves', 'gravitational-wave', 'gravitational-waves',
                ' gw ', 'ligo', 'virgo', 'kagra', 'lisa', 'taiji', 'tianqin',
                'einstein telescope', 'cosmic explorer'
            ]
            
            if any(pattern in combined_text for pattern in gw_patterns):
                filtered_papers.append(paper)
    
    return filtered_papers

def classify_submission_type(paper) -> str:
    """åˆ†ç±»æäº¤ç±»å‹"""
    primary_cat = paper.primary_category
    all_categories = paper.categories
    published = paper.published
    updated = paper.updated
    
    today = datetime.date.today()
    yesterday = today - datetime.timedelta(days=1)
    recent_dates = [today, yesterday]
    
    if published and updated:
        pub_date = published.date()
        upd_date = updated.date()
        
        # Replacement: æ›´æ–°æ—¥æœŸåœ¨æœ€è¿‘ï¼Œä½†å‘å¸ƒæ—¥æœŸä¸æ˜¯
        if upd_date in recent_dates and pub_date not in recent_dates:
            return "Replacement"
        
        # Cross-list: ä¸»ç±»åˆ«ä¸æ˜¯ç›®æ ‡ç±»åˆ«ï¼Œä½†åŒ…å«ç›®æ ‡ç±»åˆ«
        target_categories = ['gr-qc', 'astro-ph.CO', 'astro-ph.HE', 'astro-ph.IM', 'astro-ph.GA', 'astro-ph.SR', 'astro-ph.EP']
        
        if primary_cat not in target_categories:
            for cat in all_categories:
                if cat in target_categories:
                    return "Cross-list"
        
        # New: å‘å¸ƒæ—¥æœŸåœ¨æœ€è¿‘
        if pub_date in recent_dates:
            return "New"
    
    return "New"

def format_mattermost_message(papers: List[Any], date_str: str) -> str:
    """æ ¼å¼åŒ– Mattermost æ¶ˆæ¯ - ä¸¥æ ¼æŒ‰ç…§æä¾›çš„æ ¼å¼"""
    
    message_parts = []
    
    # æ ‡é¢˜
    total_papers = len(papers)
    message_parts.append(f"# ğŸ“¡ Daily GW arXiv Digest - {date_str}")
    message_parts.append(f"**Found {total_papers} gravitational wave papers**")
    message_parts.append("")
    
    # æ˜¾ç¤ºè®ºæ–‡ï¼Œä¸¥æ ¼æŒ‰ç…§æ ¼å¼ï¼ŒåŒ…å«æäº¤ç±»å‹ä¿¡æ¯
    for paper in papers[:MATTERMOST_MAX_PAPERS]:
        # arXiv ID
        arxiv_id = paper.entry_id.split('/')[-1]
        
        # å‘å¸ƒæ—¥æœŸ
        pub_date = paper.published.strftime('%d %b %Y') if paper.published else 'Unknown date'
        
        # ä½œè€…åˆ—è¡¨
        authors = [author["name"] for author in paper.authors]
        author_str = ', '.join(authors)
        
        # ç±»åˆ«ä¿¡æ¯
        primary_cat = paper.primary_category
        other_cats = [cat for cat in paper.categories if cat != primary_cat]
        
        # æ„å»ºæäº¤ä¿¡æ¯ - æ ¹æ®æäº¤ç±»å‹æ·»åŠ æ ‡è®°
        submission_info = getattr(paper, 'submission_info', {})
        submission_type = submission_info.get('type', 'new')
        
        if submission_type == 'replaced':
            arxiv_link = f"[arXiv:{arxiv_id}](https://arxiv.org/abs/{arxiv_id}) (replaced) [Submitted on {pub_date}]"
        elif submission_type == 'cross-list':
            cross_list_from = submission_info.get('cross_list_from', 'unknown')
            arxiv_link = f"[arXiv:{arxiv_id}](https://arxiv.org/abs/{arxiv_id}) (cross-list from {cross_list_from}) [Submitted on {pub_date}]"
        else:
            arxiv_link = f"[arXiv:{arxiv_id}](https://arxiv.org/abs/{arxiv_id})[Submitted on {pub_date}]"
        
        # æŒ‰ç…§æ ¼å¼è¾“å‡º
        message_parts.append(arxiv_link)
        message_parts.append(f"**{paper.title}**")
        message_parts.append(f"{author_str}")
        
        if other_cats:
            subjects_str = f"**{primary_cat}**; {', '.join(other_cats)}"
        else:
            subjects_str = f"**{primary_cat}**"
        message_parts.append(f"Subjects: {subjects_str}")
        
        # åˆ†å‰²çº¿
        message_parts.append("")
        message_parts.append("---")
        message_parts.append("")
    
    # ç»Ÿè®¡æäº¤ç±»å‹ä¿¡æ¯ï¼ˆåŸºäºç½‘é¡µæºä»£ç è§£æï¼‰
    submission_stats = {"new": 0, "cross-list": 0, "replaced": 0}
    for paper in papers:
        submission_info = getattr(paper, 'submission_info', {})
        submission_type = submission_info.get('type', 'new')
        submission_stats[submission_type] = submission_stats.get(submission_type, 0) + 1
    
    message_parts.append(f"ğŸ“Š **Summary**: {submission_stats.get('new', 0)} New â€¢ {submission_stats.get('cross-list', 0)} Cross-lists â€¢ {submission_stats.get('replaced', 0)} Replacements")
    
    if len(papers) > MATTERMOST_MAX_PAPERS:
        message_parts.append(f"ğŸ“‹ Showing top {MATTERMOST_MAX_PAPERS} of {len(papers)} papers")
    
    return "\n".join(message_parts)

def save_to_archive(all_papers, gw_papers, date_str, crawl_stats=None):
    """ä¿å­˜æ–‡ç« åˆ°å­˜æ¡£æ–‡ä»¶ï¼Œæ”¯æŒæŒ‰ç±»åˆ«åˆ†ç±»"""
    if not ENABLE_ARCHIVE:
        return
    
    # ç¡®ä¿å­˜æ¡£ç›®å½•ç»“æ„å­˜åœ¨
    archive_path = Path(ARCHIVE_DIR)
    archive_path.mkdir(parents=True, exist_ok=True)
    
    # ç¡®ä¿ archives æ ¹ç›®å½•å’Œå­ç›®å½•éƒ½å­˜åœ¨
    archives_root = Path('archives')
    archives_root.mkdir(exist_ok=True)
    
    complete_dir = archives_root / 'complete'
    filtered_dir = archives_root / 'filtered'
    
    complete_dir.mkdir(exist_ok=True)
    filtered_dir.mkdir(exist_ok=True)
    
    print(f"ğŸ“ ç¡®ä¿å­˜æ¡£ç›®å½•ç»“æ„å­˜åœ¨: {archives_root.absolute()}")
    
    # æŒ‰ç±»åˆ«åˆ†ç»„è®ºæ–‡
    gr_qc_papers = [p for p in all_papers if p.primary_category == 'gr-qc']
    astro_ph_papers = [p for p in all_papers if p.primary_category.startswith('astro-ph')]
    gw_gr_qc = [p for p in gw_papers if p.primary_category == 'gr-qc']
    gw_astro = [p for p in gw_papers if p.primary_category.startswith('astro-ph')]
    
    # 1. ä¿å­˜ç­›é€‰åçš„å¼•åŠ›æ³¢è®ºæ–‡å­˜æ¡£ - ä¿å­˜åˆ° filtered ç›®å½•
    gw_filtered_file = filtered_dir / f"gw_filtered_{date_str}.json"
    
    gw_filtered_data = {
        "date": date_str,
        "crawl_timestamp": datetime.datetime.now().isoformat(),
        "query": "cat:gr-qc OR cat:astro-ph.* (filtered for gravitational waves)",
        "summary": {
            "total_crawled": len(all_papers),
            "total_gw_papers": len(gw_papers),
            "by_category": {
                "gr-qc": {
                    "total": len(gr_qc_papers),
                    "gw_related": len(gw_gr_qc)
                },
                "astro-ph": {
                    "total": len(astro_ph_papers), 
                    "gw_related": len(gw_astro)
                }
            }
        },
        "crawl_verification": crawl_stats or {},
        "papers": []
    }
    
    for paper in gw_papers:
        # æå–æäº¤ç±»å‹ä¿¡æ¯ï¼ˆä»dtå…ƒç´ è§£æï¼‰
        submission_info = getattr(paper, 'submission_info', {})
        
        paper_data = {
            "id": paper.entry_id,
            "title": paper.title,
            "authors": [author["name"] for author in paper.authors],
            "abstract": paper.summary,
            "pdf_url": paper.pdf_url,
            "published": paper.published.isoformat() if paper.published else None,
            "updated": paper.updated.isoformat() if paper.updated else None,
            "categories": paper.categories,
            "primary_category": paper.primary_category,
            "submission_info": submission_info  # æ–°å¢ï¼šæäº¤ç±»å‹ä¿¡æ¯
        }
        gw_filtered_data["papers"].append(paper_data)
    
    with open(gw_filtered_file, 'w', encoding='utf-8') as f:
        json.dump(gw_filtered_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ å¼•åŠ›æ³¢ç­›é€‰å­˜æ¡£å·²ä¿å­˜: {gw_filtered_file}")
    
    # 2. æŒ‰ç±»åˆ«åˆ†åˆ«ä¿å­˜è¯¦ç»†å­˜æ¡£ - ç¡®ä¿åœ¨ complete ç›®å½•ä¸­
    # GR-QC å­˜æ¡£
    if gr_qc_papers:
        gr_qc_file = complete_dir / f"gr_qc_{date_str}.json"
        gr_qc_data = {
            "date": date_str,
            "category": "gr-qc",
            "url": "https://arxiv.org/list/gr-qc/new",
            "total_papers": len(gr_qc_papers),
            "gw_related_papers": len(gw_gr_qc),
            "papers": [
                {
                    "id": p.entry_id,
                    "title": p.title,
                    "authors": [a["name"] for a in p.authors],
                    "abstract": p.summary,
                    "pdf_url": p.pdf_url,
                    "categories": p.categories,
                    "is_gw_related": p in gw_papers
                }
                for p in gr_qc_papers
            ]
        }
        
        with open(gr_qc_file, 'w', encoding='utf-8') as f:
            json.dump(gr_qc_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ GR-QC å­˜æ¡£å·²ä¿å­˜: {gr_qc_file}")
    
    # Astro-Ph å­˜æ¡£
    if astro_ph_papers:
        astro_file = complete_dir / f"astro_ph_{date_str}.json"
        astro_data = {
            "date": date_str,
            "category": "astro-ph",
            "subcategories": list(set(p.primary_category for p in astro_ph_papers)),
            "total_papers": len(astro_ph_papers),
            "gw_related_papers": len(gw_astro),
            "papers": [
                {
                    "id": p.entry_id,
                    "title": p.title,
                    "authors": [a["name"] for a in p.authors],
                    "abstract": p.summary,
                    "pdf_url": p.pdf_url,
                    "categories": p.categories,
                    "primary_category": p.primary_category,
                    "is_gw_related": p in gw_papers
                }
                for p in astro_ph_papers
            ]
        }
        
        with open(astro_file, 'w', encoding='utf-8') as f:
            json.dump(astro_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ Astro-Ph å­˜æ¡£å·²ä¿å­˜: {astro_file}")
    
    print(f"âœ… å­˜æ¡£å®Œæˆ: ç­›é€‰å­˜æ¡£ + åˆ†ç±»å­˜æ¡£ (æ€»è®¡ {len(gw_papers)} ç¯‡å¼•åŠ›æ³¢è®ºæ–‡)")

def send_to_mattermost(message: str) -> bool:
    """å‘é€æ¶ˆæ¯åˆ° Mattermost"""
    if not MATTERMOST_WEBHOOK_URL:
        print("âš ï¸ MATTERMOST_WEBHOOK_URL æœªè®¾ç½®")
        return False
    
    import requests
    payload = {
        "text": message,
        "username": "GW arXiv Bot",
        "icon_emoji": ":telescope:"
    }
    
    try:
        response = requests.post(MATTERMOST_WEBHOOK_URL, json=payload, timeout=10)
        if response.status_code == 200:
            print("âœ… æ¶ˆæ¯å·²å‘é€åˆ° Mattermost")
            return True
        else:
            print(f"âŒ Mattermost å‘é€å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Mattermost å‘é€é”™è¯¯: {e}")
        return False

def main():
    """ä¸»å‡½æ•° - åˆ†åˆ«çˆ¬å– gr-qc å’Œ astro-ph è·å–å®Œæ•´æ•°æ®"""
    print("ğŸ” å¼€å§‹å®Œæ•´çˆ¬å– GR-QC å’Œ Astro-Ph æ–‡ç« ...")
    
    # åˆ†åˆ«çˆ¬å–æ¯ä¸ªç±»åˆ«ä»¥ç¡®ä¿è·å–å®Œæ•´æ•°æ®
    all_papers = []
    all_stats = []
    
    # 1. çˆ¬å– gr-qc ç±»åˆ«
    print("\n=== çˆ¬å– GR-QC ç±»åˆ« ===")
    gr_qc_papers, gr_qc_stats = fetch_category_papers("gr-qc", 100)  # gr-qc é€šå¸¸æ¯å¤© ~50 ç¯‡
    all_papers.extend(gr_qc_papers)
    all_stats.append(gr_qc_stats)
    
    # 2. çˆ¬å–å„ä¸ª astro-ph å­ç±»åˆ«
    print("\n=== çˆ¬å– Astro-Ph å­ç±»åˆ« ===")
    astro_categories = [
        "astro-ph.CO",  # å®‡å®™å­¦
        "astro-ph.HE",  # é«˜èƒ½å¤©ä½“ç‰©ç†
        "astro-ph.GA",  # é“¶æ²³ç³»å¤©ä½“ç‰©ç†
        "astro-ph.SR",  # æ’æ˜Ÿç‰©ç†
        "astro-ph.IM",  # ä»ªå™¨å’Œæ–¹æ³•
        "astro-ph.EP"   # åœ°å¤–è¡Œæ˜Ÿ
    ]
    
    for category in astro_categories:
        cat_papers, cat_stats = fetch_category_papers(category, 50)  # æ¯ä¸ªå­ç±»åˆ«æœ€å¤š 50 ç¯‡
        all_papers.extend(cat_papers)
        all_stats.append(cat_stats)
    
    print(f"\nğŸ“Š æ€»å…±è·å–åˆ° {len(all_papers)} ç¯‡æ–‡ç« ")
    
    # å»é‡ï¼ˆåŸºäº arXiv IDï¼‰
    unique_papers = {}
    for paper in all_papers:
        paper_id = paper.entry_id.split('/')[-1]
        if paper_id not in unique_papers:
            unique_papers[paper_id] = paper
    
    all_papers = list(unique_papers.values())
    print(f"ğŸ“Š å»é‡å: {len(all_papers)} ç¯‡æ–‡ç« ")
    
    # æ˜¾ç¤ºçˆ¬å–éªŒè¯æ€»ç»“
    print(f"\n=== çˆ¬å–éªŒè¯æ€»ç»“ ===")
    total_expected = sum(s.get('expected_total', 0) for s in all_stats)
    total_actual = sum(s.get('actual_crawled', 0) for s in all_stats)
    verification_passed = all(s.get('verification_passed', False) for s in all_stats if s.get('expected_total', 0) > 0)
    
    print(f"ğŸ“Š æœŸæœ›æ€»æ•°: {total_expected} ç¯‡")
    print(f"ğŸ“Š å®é™…çˆ¬å–: {total_actual} ç¯‡")
    print(f"ğŸ“Š éªŒè¯çŠ¶æ€: {'âœ… å…¨éƒ¨é€šè¿‡' if verification_passed else 'âš ï¸ éƒ¨åˆ†å¼‚å¸¸'}")
    
    # è¯¦ç»†éªŒè¯æŠ¥å‘Š
    for stats in all_stats:
        category = stats.get('category', 'Unknown')
        expected = stats.get('expected_total', 0)
        actual = stats.get('actual_crawled', 0)
        passed = stats.get('verification_passed', False)
        status = 'âœ…' if passed else 'âš ï¸'
        
        if expected > 0:
            print(f"   {status} {category}: {actual}/{expected} ç¯‡")
        else:
            print(f"   âšª {category}: {actual} ç¯‡ (æ— é¡µé¢æ€»æ•°)")
            
    crawl_verification = {
        "total_expected": total_expected,
        "total_actual": total_actual,
        "verification_passed": verification_passed,
        "category_details": all_stats
    }
    
    # ç­›é€‰ä»Šå¤©çš„æ–‡ç« 
    print("ğŸ“… æ­£åœ¨ç­›é€‰ä»Šå¤©çš„æ–‡ç« ...")
    today_papers = filter_today_papers(all_papers)
    print(f"âœ… ä»Šå¤©çš„æ–‡ç« : {len(today_papers)} ç¯‡")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡ä»Šå¤©çš„æ–‡ç« 
    gr_qc_today = [p for p in today_papers if p.primary_category == 'gr-qc']
    astro_ph_today = [p for p in today_papers if p.primary_category.startswith('astro-ph')]
    
    print(f"ğŸ“Š ä»Šå¤©çš„æ–‡ç« åˆ†å¸ƒ:")
    print(f"   gr-qc: {len(gr_qc_today)} ç¯‡")
    print(f"   astro-ph: {len(astro_ph_today)} ç¯‡")
    
    # ç­›é€‰å¼•åŠ›æ³¢ç›¸å…³æ–‡ç« 
    print("ğŸ” æ­£åœ¨ç­›é€‰å¼•åŠ›æ³¢ç›¸å…³æ–‡ç« ...")
    gw_papers = filter_gravitational_wave_papers(today_papers)
    print(f"âœ… ç­›é€‰å‡º {len(gw_papers)} ç¯‡å¼•åŠ›æ³¢ç›¸å…³æ–‡ç« ")
    
    if len(gw_papers) == 0:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¼•åŠ›æ³¢ç›¸å…³æ–‡ç« ")
        return [], ""
    
    # ç»Ÿè®¡æäº¤ç±»å‹ï¼ˆåŸºäºç½‘é¡µæºä»£ç è§£æï¼‰
    submission_stats = {"new": 0, "cross-list": 0, "replaced": 0}
    for paper in gw_papers:
        submission_info = getattr(paper, 'submission_info', {})
        submission_type = submission_info.get('type', 'new')
        submission_stats[submission_type] = submission_stats.get(submission_type, 0) + 1
    
    print(f"ğŸ“Š å¼•åŠ›æ³¢è®ºæ–‡æäº¤ç±»å‹:")
    print(f"   ğŸ†• New: {submission_stats.get('new', 0)}")
    print(f"   ğŸ”„ Cross-lists: {submission_stats.get('cross-list', 0)}")
    print(f"   ğŸ”„ Replacements: {submission_stats.get('replaced', 0)}")
    
    # è·å–æ—¥æœŸ
    date_str = datetime.date.today().strftime('%Y-%m-%d')
    
    # ä¿å­˜åˆ°å­˜æ¡£ï¼ˆåŒ…å«çˆ¬å–éªŒè¯ç»Ÿè®¡ï¼‰
    if ENABLE_ARCHIVE:
        save_to_archive(today_papers, gw_papers, date_str, crawl_verification)
    
    # ç”Ÿæˆ digest æ–‡ä»¶
    print("ğŸ“ æ­£åœ¨ç”Ÿæˆ digest.md...")
    with open('digest.md', 'w', encoding='utf-8') as f:
        f.write(f"# Complete Daily GW arXiv Digest - {date_str}\n\n")
        f.write(f"**æ€»çˆ¬å–æ–‡ç« **: {len(all_papers)} ç¯‡  \n")
        f.write(f"**ä»Šå¤©çš„æ–‡ç« **: {len(today_papers)} ç¯‡  \n")
        f.write(f"  - gr-qc: {len(gr_qc_today)} ç¯‡  \n")
        f.write(f"  - astro-ph: {len(astro_ph_today)} ç¯‡  \n")
        f.write(f"**å¼•åŠ›æ³¢ç›¸å…³**: {len(gw_papers)} ç¯‡  \n")
        f.write(f"**æäº¤ç±»å‹**: ğŸ†• {submission_stats.get('new', 0)} New â€¢ ğŸ”„ {submission_stats.get('cross-list', 0)} Cross-lists â€¢ ğŸ”„ {submission_stats.get('replaced', 0)} Replacements  \n")
        f.write("\n")
        
        for i, paper in enumerate(gw_papers, 1):
            arxiv_id = paper.entry_id.split('/')[-1]
            pub_date = paper.published.strftime('%d %b %Y') if paper.published else 'Unknown'
            
            f.write(f"## {i}. {paper.title}\n\n")
            f.write(f"**arXiv**: [{arxiv_id}](https://arxiv.org/abs/{arxiv_id})  \n")
            f.write(f"**Authors**: {', '.join(author['name'] for author in paper.authors)}  \n")
            f.write(f"**Date**: {pub_date}  \n")
            f.write(f"**Categories**: {', '.join(paper.categories)}  \n")
            submission_info = getattr(paper, 'submission_info', {})
            f.write(f"**Type**: {submission_info.get('details', 'new submission')}  \n\n")
            f.write(f"**Abstract**: {paper.summary}  \n\n")
            f.write("---\n\n")
    
    print("âœ… digest.md ç”Ÿæˆå®Œæˆ")
    
    return gw_papers, date_str

if __name__ == "__main__":
    papers, date_str = main()
    
    # ç”Ÿæˆ Mattermost æ¶ˆæ¯
    if papers:
        print("\nğŸ“± å‡†å¤‡ Mattermost æ¶ˆæ¯...")
        message = format_mattermost_message(papers, date_str)
        
        # ä¿å­˜é¢„è§ˆ
        with open('mattermost_preview.md', 'w', encoding='utf-8') as f:
            f.write(message)
        print(f"ğŸ“‹ Mattermost é¢„è§ˆå·²ä¿å­˜ ({len(message)} å­—ç¬¦)")
        
        # å¯é€‰ï¼šå‘é€åˆ° Mattermost
        # send_to_mattermost(message)
