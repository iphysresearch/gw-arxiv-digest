name: System Health Check

on:
  schedule:
    - cron: '0 6 * * *'  # ÊØèÂ§© UTC 6:00 ËøêË°åÂÅ•Â∫∑Ê£ÄÊü•
  workflow_dispatch:  # ÊîØÊåÅÊâãÂä®Ëß¶Âèë

permissions:
  contents: read

jobs:
  health-check:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python with caching
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: System health verification
        id: health-check
        run: |
          echo "üè• Daily System Health Check"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          
          DATE_STR=$(date +%Y-%m-%d)
          YESTERDAY=$(date -d "yesterday" +%Y-%m-%d 2>/dev/null || date -v-1d +%Y-%m-%d)
          
          # Ê£ÄÊü•Ê†∏ÂøÉÊñá‰ª∂
          echo "üîß Core components check:"
          
          COMPONENTS=(
            "scripts/arxiv_web_scraper.py:ÁΩëÈ°µÁà¨Ëô´Ê®°Âùó"
            "scripts/fetch_complete_gw.py:‰∏ªÁà¨ÂèñËÑöÊú¨" 
            "scripts/send_complete_gw.py:MattermostÂèëÈÄÅËÑöÊú¨"
            "requirements.txt:‰æùËµñÈÖçÁΩÆ"
            "Makefile:ÊûÑÂª∫Á≥ªÁªü"
          )
          
          for component in "${COMPONENTS[@]}"; do
            file=${component%%:*}
            name=${component##*:}
            if [ -f "$file" ]; then
              echo "  ‚úÖ $name"
            else
              echo "  ‚ùå $name (missing: $file)"
            fi
          done
          
          # Ê£ÄÊü•Â≠òÊ°£Áä∂ÊÄÅ
          echo ""
          echo "üìÅ Archive status check:"
          
          if [ -f "archives/filtered/gw_filtered_$DATE_STR.json" ]; then
            SIZE=$(stat -c%s "archives/filtered/gw_filtered_$DATE_STR.json" 2>/dev/null || echo "0")
            echo "  ‚úÖ Today's GW filtered archive ($SIZE bytes)"
          else
            echo "  ‚ö†Ô∏è No today's GW filtered archive"
          fi
          
          if [ -f "archives/filtered/gw_filtered_$YESTERDAY.json" ]; then
            SIZE=$(stat -c%s "archives/filtered/gw_filtered_$YESTERDAY.json" 2>/dev/null || echo "0")
            echo "  ‚úÖ Yesterday's GW filtered archive ($SIZE bytes)"
          else
            echo "  ‚ö†Ô∏è No yesterday's GW filtered archive"
          fi
          
          # Ê£ÄÊü•Â≠òÊ°£ÁõÆÂΩïÁªìÊûÑ
          COMPLETE_COUNT=$(find archives/complete -name "*.json" -type f 2>/dev/null | wc -l)
          FILTERED_COUNT=$(find archives/filtered -name "*.json" -type f 2>/dev/null | wc -l)
          
          echo "  üìä Complete archives: $COMPLETE_COUNT files"
          echo "  üìä Filtered archives: $FILTERED_COUNT files"
          
          # ÊµãËØïÁΩëÁªúËøûÊé•
          echo ""
          echo "üåê Network connectivity check:"
          
          if curl -s --head "https://arxiv.org/list/gr-qc/new" >/dev/null; then
            echo "  ‚úÖ arXiv GR-QC accessible"
          else
            echo "  ‚ùå arXiv GR-QC not accessible"
          fi
          
          if curl -s --head "https://arxiv.org/list/astro-ph.CO/new" >/dev/null; then
            echo "  ‚úÖ arXiv Astro-Ph accessible"
          else
            echo "  ‚ùå arXiv Astro-Ph not accessible"
          fi
          
          echo ""
          echo "üè• Health check completed"
          
      - name: Quick crawler test
        id: crawler-test
        continue-on-error: true
        run: |
          echo "üß™ Testing web crawler functionality..."
          
          # ÁÆÄÂçïÁöÑÁà¨Ëô´ÂäüËÉΩÊµãËØï
          python3 -c "
import sys
sys.path.append('scripts')

try:
    from arxiv_web_scraper import ArxivWebScraper
    scraper = ArxivWebScraper()
    
    # ÊµãËØïÊäìÂèñÂ∞ëÈáèËÆ∫Êñá
    papers, stats = scraper.fetch_category_new('gr-qc')
    
    if len(papers) > 0:
        print(f'‚úÖ Web crawler functional: {len(papers)} papers crawled')
        print(f'üìä Verification: {\"‚úÖ PASS\" if stats.get(\"verification_passed\") else \"‚ö†Ô∏è ISSUE\"}')
        print(f'üìä Expected: {stats.get(\"expected_total\", 0)}')
        print(f'üìä Actual: {stats.get(\"actual_crawled\", 0)}')
    else:
        print('‚ùå Web crawler test failed: no papers retrieved')
        
except Exception as e:
    print(f'‚ùå Web crawler test error: {e}')
" || echo "‚ùå Crawler test failed"

      - name: Send health status to Mattermost
        if: always()
        env:
          MATTERMOST_WEBHOOK_URL: ${{ secrets.MATTERMOST_WEBHOOK_URL }}
        run: |
          if [ ! -z "$MATTERMOST_WEBHOOK_URL" ]; then
            DATE_STR=$(date +%Y-%m-%d)
            HOUR=$(date +%H)
            
            # Ê£ÄÊü•‰ªäÂ§©ÊòØÂê¶ÊúâÂ≠òÊ°£
            if [ -f "archives/filtered/gw_filtered_$DATE_STR.json" ]; then
              DIGEST_STATUS="‚úÖ Generated"
              SIZE=$(stat -c%s "archives/filtered/gw_filtered_$DATE_STR.json" 2>/dev/null || echo "0")
            else
              DIGEST_STATUS="‚ö†Ô∏è Pending"
              SIZE="0"
            fi
            
            echo "üì± Sending health check to Mattermost..."
            curl -s -X POST -H 'Content-Type: application/json' \
              -d "{
                \"text\": \"üè• Daily Health Check - $DATE_STR\\n\\nüìä System Status:\\n- Digest today: $DIGEST_STATUS ($SIZE bytes)\\n- Crawler test: ${{ job.status == 'success' && '‚úÖ PASS' || '‚ö†Ô∏è ISSUE' }}\\n- Next check: UTC $(($HOUR + 1)):00\\n\\nüîß All core components verified\",
                \"username\": \"GW System Monitor\", 
                \"icon_emoji\": \":health_worker:\"
              }" \
              $MATTERMOST_WEBHOOK_URL || echo "‚ö†Ô∏è Health status send failed"
          fi
