#!/usr/bin/env python3
"""
å®Œæ•´ç³»ç»ŸéªŒè¯è„šæœ¬
éªŒè¯ç½‘é¡µçˆ¬è™«è‡ªæ£€åŠŸèƒ½å’Œåˆ†ç±»å­˜æ¡£åŠŸèƒ½
"""

import sys
import os
import json
import datetime
from pathlib import Path

# æ·»åŠ è„šæœ¬ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / 'scripts'))

def test_web_scraper_import():
    """æµ‹è¯•ç½‘é¡µçˆ¬è™«æ¨¡å—å¯¼å…¥"""
    print("ğŸ§ª æµ‹è¯•ç½‘é¡µçˆ¬è™«æ¨¡å—å¯¼å…¥...")
    try:
        from arxiv_web_scraper import ArxivWebScraper, Client, Search
        print("âœ… ç½‘é¡µçˆ¬è™«æ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except ImportError as e:
        print(f"âŒ ç½‘é¡µçˆ¬è™«æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_single_category_crawl():
    """æµ‹è¯•å•ä¸ªç±»åˆ«çˆ¬å–å’Œè‡ªæ£€åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•å•ä¸ªç±»åˆ«çˆ¬å–å’Œè‡ªæ£€åŠŸèƒ½...")
    
    try:
        from arxiv_web_scraper import ArxivWebScraper
        scraper = ArxivWebScraper()
        
        # æµ‹è¯• gr-qc ç±»åˆ«
        print("ğŸ“¡ æµ‹è¯• gr-qc ç±»åˆ«...")
        papers, stats = scraper.fetch_category_new('gr-qc')
        
        print(f"ğŸ“Š çˆ¬å–ç»“æœ: {len(papers)} ç¯‡è®ºæ–‡")
        print(f"ğŸ“Š æœŸæœ›æ•°é‡: {stats.get('expected_total', 'unknown')} ç¯‡")
        print(f"ğŸ“Š é¡µé¢ä¿¡æ¯: {stats.get('page_source_info', 'none')}")
        print(f"ğŸ” éªŒè¯çŠ¶æ€: {'âœ… é€šè¿‡' if stats.get('verification_passed') else 'âš ï¸ å¼‚å¸¸'}")
        
        # éªŒè¯è‡ªæ£€åŠŸèƒ½
        if stats.get('expected_total', 0) > 0:
            expected = stats['expected_total']
            actual = stats['actual_crawled']
            tolerance = 5
            
            if abs(expected - actual) <= tolerance:
                print("âœ… è‡ªæ£€åŠŸèƒ½æ­£å¸¸: çˆ¬å–æ•°é‡ä¸é¡µé¢æ˜¾ç¤ºä¸€è‡´")
                return True, papers, stats
            else:
                print(f"âš ï¸ è‡ªæ£€å‘ç°æ•°é‡å·®å¼‚: é¡µé¢æ˜¾ç¤º{expected}ç¯‡ï¼Œå®é™…çˆ¬å–{actual}ç¯‡")
                return False, papers, stats
        else:
            print("âš ï¸ æ— æ³•ä»é¡µé¢è·å–æ€»æ•°ä¿¡æ¯")
            return len(papers) > 0, papers, stats
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False, [], {}

def test_complete_system():
    """æµ‹è¯•å®Œæ•´ç³»ç»Ÿ"""
    print("\nğŸ§ª æµ‹è¯•å®Œæ•´ç³»ç»Ÿ...")
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å¯ç”¨å­˜æ¡£
        os.environ['ENABLE_ARCHIVE'] = 'true'
        os.environ['ARCHIVE_DIR'] = 'archives/complete'
        
        from fetch_complete_gw import main
        
        print("ğŸš€ è¿è¡Œå®Œæ•´ç³»ç»Ÿæµ‹è¯•...")
        papers, date_str = main()
        
        print(f"âœ… å®Œæ•´ç³»ç»Ÿæµ‹è¯•å®Œæˆ: è·å¾— {len(papers)} ç¯‡å¼•åŠ›æ³¢è®ºæ–‡")
        return True, papers, date_str
        
    except Exception as e:
        print(f"âŒ å®Œæ•´ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False, [], ""

def verify_archive_structure():
    """éªŒè¯å­˜æ¡£æ–‡ä»¶ç»“æ„"""
    print("\nğŸ§ª éªŒè¯å­˜æ¡£æ–‡ä»¶ç»“æ„...")
    
    date_str = datetime.date.today().strftime('%Y-%m-%d')
    archive_dir = Path('archives/complete')
    
    expected_files = [
        f"filtered/gw_filtered_{date_str}.json",  # ç­›é€‰åçš„å¼•åŠ›æ³¢è®ºæ–‡
        f"complete/gr_qc_{date_str}.json",        # å®Œæ•´GR-QCè®ºæ–‡
        f"complete/astro_ph_{date_str}.json"      # å®Œæ•´Astro-Phè®ºæ–‡
    ]
    
    found_files = []
    missing_files = []
    
    for relative_filename in expected_files:
        file_path = Path('archives') / relative_filename
        filename = relative_filename.split('/')[-1]  # è·å–æ–‡ä»¶åéƒ¨åˆ†
        
        if file_path.exists():
            found_files.append(relative_filename)
            print(f"âœ… æ‰¾åˆ°å­˜æ¡£æ–‡ä»¶: {relative_filename}")
            
            # éªŒè¯æ–‡ä»¶å†…å®¹
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                if 'gw_filtered' in filename:
                    # éªŒè¯å¼•åŠ›æ³¢ç­›é€‰å­˜æ¡£æ–‡ä»¶
                    if 'crawl_verification' in data:
                        print(f"   ğŸ” åŒ…å«çˆ¬å–éªŒè¯ä¿¡æ¯")
                    if 'summary' in data:
                        summary = data['summary']
                        print(f"   ğŸ“Š æ€»çˆ¬å–: {summary.get('total_crawled', 0)} ç¯‡")
                        print(f"   ğŸ“Š å¼•åŠ›æ³¢: {summary.get('total_gw_papers', 0)} ç¯‡")
                    
                elif 'gr_qc' in filename:
                    # éªŒè¯ GR-QC å­˜æ¡£
                    total = data.get('total_papers', 0)
                    gw_related = data.get('gw_related_papers', 0) 
                    print(f"   ğŸ“Š GR-QC: {total} ç¯‡æ€»è®¡ï¼Œ{gw_related} ç¯‡å¼•åŠ›æ³¢ç›¸å…³")
                    
                elif 'astro_ph' in filename:
                    # éªŒè¯ Astro-Ph å­˜æ¡£
                    total = data.get('total_papers', 0)
                    gw_related = data.get('gw_related_papers', 0)
                    subcats = data.get('subcategories', [])
                    print(f"   ğŸ“Š Astro-Ph: {total} ç¯‡æ€»è®¡ï¼Œ{gw_related} ç¯‡å¼•åŠ›æ³¢ç›¸å…³")
                    print(f"   ğŸ“Š å­ç±»åˆ«: {len(subcats)} ä¸ª")
                    
            except Exception as e:
                print(f"   âŒ æ–‡ä»¶æ ¼å¼éªŒè¯å¤±è´¥: {e}")
                
        else:
            missing_files.append(relative_filename)
            print(f"âš ï¸ ç¼ºå°‘å­˜æ¡£æ–‡ä»¶: {relative_filename}")
    
    if len(found_files) == len(expected_files):
        print("âœ… æ‰€æœ‰å­˜æ¡£æ–‡ä»¶éƒ½å·²åˆ›å»ºä¸”æ ¼å¼æ­£ç¡®")
        return True
    else:
        print(f"âš ï¸ æ‰¾åˆ° {len(found_files)}/{len(expected_files)} ä¸ªå­˜æ¡£æ–‡ä»¶")
        return False

def verify_crawl_targets():
    """éªŒè¯çˆ¬å–ç›®æ ‡è¾¾æˆæƒ…å†µ"""
    print("\nğŸ§ª éªŒè¯çˆ¬å–ç›®æ ‡è¾¾æˆæƒ…å†µ...")
    
    date_str = datetime.date.today().strftime('%Y-%m-%d')
    filtered_file = Path('archives/filtered') / f"gw_filtered_{date_str}.json"
    
    if not filtered_file.exists():
        print("âŒ å¼•åŠ›æ³¢ç­›é€‰å­˜æ¡£æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒéªŒè¯")
        return False
    
    try:
        with open(filtered_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        verification = data.get('crawl_verification', {})
        details = verification.get('category_details', [])
        
        gr_qc_count = sum(d.get('actual_crawled', 0) for d in details if d.get('category', '') == 'gr-qc')
        astro_details = [d for d in details if d.get('category', '').startswith('astro-ph')]
        astro_passed = sum(1 for d in astro_details if d.get('verification_passed', False))
        astro_total = len(astro_details)
        
        print(f"ğŸ“Š GR-QC: {gr_qc_count} ç¯‡ (ç›®æ ‡: 47ç¯‡)")
        print(f"ğŸ“Š Astro-Phå­ç±»åˆ«éªŒè¯: {astro_passed}/{astro_total} ä¸ªé€šè¿‡")
        
        for d in astro_details[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå­ç±»åˆ«
            category = d.get('category', '?')
            expected = d.get('expected_total', 0)
            actual = d.get('actual_crawled', 0)
            passed = d.get('verification_passed', False)
            print(f"   {category}: {actual}/{expected} ç¯‡ {'âœ…' if passed else 'âš ï¸'}")
        
        if len(astro_details) > 3:
            print(f"   ... å…¶ä½™ {len(astro_details)-3} ä¸ªå­ç±»åˆ«")
        
        gr_qc_ok = 35 <= gr_qc_count <= 60
        astro_ok = astro_passed == astro_total  # æ‰€æœ‰å­ç±»åˆ«éªŒè¯éƒ½é€šè¿‡
        
        print(f"ğŸ¯ GR-QC ç›®æ ‡è¾¾æˆ: {'âœ…' if gr_qc_ok else 'âš ï¸'}")
        print(f"ğŸ¯ Astro-Phå­ç±»åˆ«éªŒè¯: {'âœ…' if astro_ok else 'âš ï¸'} ({astro_passed}/{astro_total})")
        
        total_expected = verification.get('total_expected', 0)
        total_actual = verification.get('total_actual', 0)
        verification_passed = verification.get('verification_passed', False)
        
        print(f"ğŸ” é¡µé¢éªŒè¯: {'âœ… é€šè¿‡' if verification_passed else 'âš ï¸ å¼‚å¸¸'}")
        print(f"ğŸ” æ€»æ•°å¯¹æ¯”: {total_actual}/{total_expected} ç¯‡")
        
        return gr_qc_ok and astro_ok and verification_passed
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å®Œæ•´ç³»ç»ŸéªŒè¯æµ‹è¯•")
    print("=" * 50)
    
    success_count = 0
    total_tests = 5
    
    # æµ‹è¯•1: æ¨¡å—å¯¼å…¥
    if test_web_scraper_import():
        success_count += 1
    
    # æµ‹è¯•2: å•ä¸ªç±»åˆ«çˆ¬å–å’Œè‡ªæ£€
    crawl_success, papers, stats = test_single_category_crawl()
    if crawl_success:
        success_count += 1
    
    # æµ‹è¯•3: å®Œæ•´ç³»ç»Ÿ
    system_success, all_papers, date_str = test_complete_system()
    if system_success:
        success_count += 1
    
    # æµ‹è¯•4: å­˜æ¡£æ–‡ä»¶ç»“æ„
    if verify_archive_structure():
        success_count += 1
    
    # æµ‹è¯•5: çˆ¬å–ç›®æ ‡éªŒè¯
    if verify_crawl_targets():
        success_count += 1
    
    # ç»“æœæ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ‰ ç³»ç»ŸéªŒè¯å®Œæˆ!")
    print(f"ğŸ“Š é€šè¿‡æµ‹è¯•: {success_count}/{total_tests}")
    
    if success_count == total_tests:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
        print("\nğŸ¯ å…³é”®åŠŸèƒ½éªŒè¯:")
        print("  âœ… ç½‘é¡µçˆ¬è™«è‡ªæ£€åŠŸèƒ½æ­£å¸¸")
        print("  âœ… æŒ‰ç±»åˆ«åˆ†ç±»å­˜æ¡£åŠŸèƒ½æ­£å¸¸") 
        print("  âœ… çˆ¬å–æ•°é‡éªŒè¯åŠŸèƒ½æ­£å¸¸")
        print("  âœ… å­˜æ¡£æ–‡ä»¶ç»“æ„å®Œæ•´")
        return 0
    else:
        print(f"âš ï¸ {total_tests - success_count} ä¸ªæµ‹è¯•æœªé€šè¿‡ï¼Œç³»ç»Ÿéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
